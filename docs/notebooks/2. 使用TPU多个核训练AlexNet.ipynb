{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiCore Training AlexNet on Fashion MNIST \n",
    "\n",
    "一块Cloud TPU上面包含了8个核，只用其中一个极大限制了Cloud TPU的能力。我们看看如何发挥8核的威力吧。\n",
    "\n",
    "\n",
    "* 参考 https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/multi-core-alexnet-fashion-mnist.ipynb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集 & 模型\n",
    "\n",
    "数据集：Fashion MNIST\n",
    "\n",
    "模型：AlexNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用多个Cloud TPU核\n",
    "\n",
    "使用多核训练模型和单核还是有区别的，比如必须使用多进程，每个Cloud TPU核对应一个进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process0  is using TPU:0\n",
      "Process 6 is using TPU:6\n",
      "Process 4 is using TPU:4\n",
      "Process 3 is using TPU:3\n",
      "Process 1 is using TPU:1\n",
      "Process 7 is using TPU:7\n",
      "Process 5 is using TPU:5\n",
      "Process 2 is using TPU:2\n"
     ]
    }
   ],
   "source": [
    "# \"Map function\": acquires a corresponding Cloud TPU core, creates a tensor on it,\n",
    "# and prints its core\n",
    "def simple_map_fn(index, flags):\n",
    "    \"\"\"\n",
    "    index: index of process\n",
    "    \"\"\"\n",
    "    # Sets a common random seed - both for initialization and ensuring graph is the same\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
    "    device = xm.xla_device()  # no explicitly specify TPU core, thanks to xmp.spawn()\n",
    "\n",
    "    # Creates a tensor on this process's device\n",
    "    t = torch.randn((2, 2), device=device)\n",
    "\n",
    "    print(\"Process\", index ,\"is using\", xm.xla_real_devices([str(device)])[0])\n",
    "\n",
    "    # Barrier to prevent master from exiting before workers connect.\n",
    "    xm.rendezvous('init')  # 防止主进程先exist\n",
    "\n",
    "# Spawns eight of the map functions, one for each of the eight cores on\n",
    "# the Cloud TPU\n",
    "flags = {}\n",
    "# Note: Colab only supports start_method='fork'\n",
    "xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ `spawn()` 文档] [here](http://pytorch.org/xla/#torch_xla.distributed.xla_multiprocessing.spawn)， `spawn()` 接收一个（map）函数、函数参数列表（tuple类型）、要创建的进程数量（`nprocs`）以及创建进程的方式（`fork`或`spawn`）。\n",
    "\n",
    "`xmp.spawn()` 创建了8个进程，每个进程对应一个Cloud TPU核，每个进程上都调用 `simple_map_fn()` 。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Aside on Context\n",
    "\n",
    "上面每个进程是如何知道自己拿到的是哪个Cloud TPU核的？答案是context。\n",
    "\n",
    "Cloud TPU通过一个隐式的stateful context来管理算子/计算操作， `xmp.spawn()` 函数创建了一个多进程context，每个子进程都可以访问这个context。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意：如果你使用了多进程的context，就不能再创建单进程的context了，二者不能混用，会冲突！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't mix these!\n",
    "# Only one type of context per Colab!\n",
    "# Warning: uncommenting the below and running this cell will cause a runtime error!\n",
    "\n",
    "# device = xm.xla_device()  # Requires a single process context\n",
    "\n",
    "# xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')  # Requires a multiprocess context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二点要注意的：每个进程的计算任务要相同。不能在`simple_map_fn`中为不同的进程设置不同的计算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't perform different computations on different processes!\n",
    "# Warning: uncommenting the below and running this cell will likely hang your Colab!\n",
    "# def simple_map_fn(index, flags):\n",
    "#   torch.manual_seed(1234)\n",
    "#   device = xm.xla_device()  \n",
    "\n",
    "#   if xm.is_master_ordinal():\n",
    "#     t = torch.randn((2, 2), device=device)  # Divergent Cloud TPU computation!\n",
    "\n",
    "\n",
    "# xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有每个Cloud TPU 核的计算任务完全一致，context才能正确管理它们。但是我们可以在每个进程中执行不同的CPU计算任务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6989, -0.0987],\n",
      "        [ 0.7337, -0.9071]], device='xla:1')\n"
     ]
    }
   ],
   "source": [
    "# Common Cloud TPU computation but different CPU computation is OK\n",
    "def simple_map_fn(index, flags):\n",
    "  torch.manual_seed(1234)\n",
    "  device = xm.xla_device()  \n",
    "\n",
    "  t = torch.randn((2, 2), device=device)  # Common Cloud TPU computation\n",
    "  out = str(t)  # Each process uses the XLA tensors the same way\n",
    "\n",
    "  if xm.is_master_ordinal():  # Divergent CPU-only computation (no XLA tensors beyond this point!)\n",
    "    print(out)\n",
    "\n",
    "  # Barrier to prevent master from exiting before workers connect.\n",
    "  xm.rendezvous('init')\n",
    "\n",
    "\n",
    "xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多核训练\n",
    "\n",
    "定义一个可以在8个Cloud TPU核上训练AlexNet的函数:\n",
    "\n",
    "- **Setup**: 每个进程的随机数种子都相同\n",
    "- **Dataloading**: 每个进程都有一份数据集备份，但是数据集sampling的结果不重复\n",
    "- **Network creation**: 每个进程都有一份模型备份，由于每个进程的速技术相同，所以模型权重的值也完全相同\n",
    "- **Training** and **Evaluation**: Training and evaluation occur as usual but use a ParallelLoader.\n",
    "\n",
    "实际上就是数据并行，类比`DistributedDataParallel`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import time\n",
    "\n",
    "def map_fn(index, flags):\n",
    "    ## Setup \n",
    "\n",
    "    # Sets a common random seed - both for initialization and ensuring graph is the same\n",
    "    torch.manual_seed(flags['seed'])\n",
    "\n",
    "    # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
    "    device = xm.xla_device()  \n",
    "\n",
    "\n",
    "    ## Dataloader construction\n",
    "\n",
    "    # Creates the transform for the raw Torchvision data\n",
    "    # See https://pytorch.org/docs/stable/torchvision/models.html for normalization\n",
    "    # Pre-trained TorchVision models expect RGB (3 x H x W) images\n",
    "    # H and W should be >= 224\n",
    "    # Loaded into [0, 1] and normalized as follows:\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]\n",
    "                                    )\n",
    "    to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))\n",
    "    resize = transforms.Resize((224, 224))\n",
    "    my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize])\n",
    "\n",
    "    # Downloads train and test datasets\n",
    "    # Note: master goes first and downloads the dataset only once (xm.rendezvous)\n",
    "    #   all the other workers wait for the master to be done downloading.\n",
    "    if not xm.is_master_ordinal():\n",
    "        xm.rendezvous('download_only_once')\n",
    "    \n",
    "    # Only master process load FashionMNIST dataset\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        \"/tmp/fashionmnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=my_transform\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST(\n",
    "        \"/tmp/fashionmnist\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=my_transform\n",
    "    )\n",
    "\n",
    "    if xm.is_master_ordinal():\n",
    "        xm.rendezvous('download_only_once')\n",
    "\n",
    "    # Creates the (distributed) train sampler, which let this process only access\n",
    "    # its portion of the training dataset.\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        test_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Creates dataloaders, which load data in batches\n",
    "    # Note: test loader is not shuffled or sampled\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=flags['batch_size'],\n",
    "        sampler=train_sampler,\n",
    "        num_workers=flags['num_workers'],\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=flags['batch_size'],\n",
    "        sampler=test_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=flags['num_workers'],\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    ## Network, optimizer, and loss function creation\n",
    "\n",
    "    # Creates AlexNet for 10 classes\n",
    "    # Note: each process has its own identical copy of the model\n",
    "    #  Even though each model is created independently, they're also\n",
    "    #  created in the same way.\n",
    "    net = torchvision.models.alexnet(num_classes=10).to(device).train()\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    ## Trains\n",
    "    train_start = time.time()\n",
    "    for epoch in range(flags['num_epochs']):\n",
    "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)  # Note \n",
    "        for batch_num, batch in enumerate(para_train_loader):\n",
    "            data, targets = batch   # no to.(device)?\n",
    "#             print(data.size())\n",
    "            # batch_size\n",
    "\n",
    "            # Acquires the network's best guesses at each class\n",
    "            output = net(data)\n",
    "\n",
    "            # Computes loss\n",
    "            loss = loss_fn(output, targets)\n",
    "\n",
    "            # Updates model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Note: optimizer_step uses the implicit Cloud TPU context to\n",
    "            #  coordinate and synchronize gradient updates across processes.\n",
    "            #  This means that each process's network has the same weights after\n",
    "            #  this is called.\n",
    "            # Warning: this coordination requires the actions performed in each \n",
    "            #  process are the same. In more technical terms, the graph that\n",
    "            #  PyTorch/XLA generates must be the same across processes. \n",
    "            xm.optimizer_step(optimizer)  # Note: barrier=True not needed when using ParallelLoader \n",
    "\n",
    "    elapsed_train_time = time.time() - train_start\n",
    "    print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time) \n",
    "\n",
    "\n",
    "    ## Evaluation\n",
    "    # Sets net to eval and no grad context \n",
    "    net.eval()\n",
    "    eval_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        total_guesses = 0\n",
    "\n",
    "        para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
    "        for batch_num, batch in enumerate(para_train_loader):\n",
    "            data, targets = batch\n",
    "\n",
    "            # Acquires the network's best guesses at each class\n",
    "            output = net(data)\n",
    "            best_guesses = torch.argmax(output, 1)\n",
    "\n",
    "            # Updates running statistics\n",
    "            num_correct += torch.eq(targets, best_guesses).sum().item()\n",
    "            total_guesses += flags['batch_size']\n",
    "\n",
    "    elapsed_eval_time = time.time() - eval_start\n",
    "    print(\"Process\", index, \"finished evaluation. Evaluation time was:\", elapsed_eval_time)\n",
    "    print(\"Process\", index, \"guessed\", num_correct, \"of\", total_guesses, \"correctly for\", num_correct/total_guesses * 100, \"% accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessProcessProcessProcessProcessProcessProcessProcess       6 132745 0      finished training. Train time was: finished training. Train time was:finished training. Train time was:finished training. Train time was:finished training. Train time was:finished training. Train time was:finished training. Train time was: finished training. Train time was:       43.0919952392578142.82916164398193443.0883066654205343.0956840515136743.0569190979003943.1000208854675344.9737517833709743.09987211227417\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Process 0 finished evaluation. Evaluation time was: 7.351853132247925\n",
      "Process 0 guessed 1080 of 1248 correctly for 86.53846153846155 % accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 06:17:09.358847: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Connection reset by peer\" and grpc_error_string = \"{\"created\":\"@1657261029.358613665\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Connection reset by peer\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.358893: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Connection reset by peer\" and grpc_error_string = \"{\"created\":\"@1657261029.358635196\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Connection reset by peer\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.358921: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Socket closed\" and grpc_error_string = \"{\"created\":\"@1657261029.358729867\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.359333: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Socket closed\" and grpc_error_string = \"{\"created\":\"@1657261029.359148619\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.359314: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Connection reset by peer\" and grpc_error_string = \"{\"created\":\"@1657261029.359124410\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Connection reset by peer\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.359489: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Connection reset by peer\" and grpc_error_string = \"{\"created\":\"@1657261029.359338467\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Connection reset by peer\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.359485: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Connection reset by peer\" and grpc_error_string = \"{\"created\":\"@1657261029.359177012\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Connection reset by peer\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.359522: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Socket closed\" and grpc_error_string = \"{\"created\":\"@1657261029.359333083\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.359571: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Socket closed\" and grpc_error_string = \"{\"created\":\"@1657261029.359390402\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.359582: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Socket closed\" and grpc_error_string = \"{\"created\":\"@1657261029.359161307\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\n",
      "2022-07-08 06:17:09.360009: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"UNAVAILABLE: Socket closed\" and grpc_error_string = \"{\"created\":\"@1657261029.359820285\",\"description\":\"Error received from peer ipv4:127.0.0.1:51011\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\n"
     ]
    }
   ],
   "source": [
    "# Configures training (and evaluation) parameters\n",
    "flags['batch_size'] = 32  # batch_size per device?\n",
    "flags['num_workers'] = 8\n",
    "flags['num_epochs'] = 1\n",
    "flags['seed'] = 1234\n",
    "\n",
    "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用多核比单核训练快多了，毕竟batch size是原来的8倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

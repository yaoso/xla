{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb3e369",
   "metadata": {},
   "source": [
    "## Getting Started with PyTorch on Cloud TPUs\n",
    "\n",
    "* https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b0c9e",
   "metadata": {},
   "source": [
    "PyTorch/XLA 连接了PyTorch和Cloud TPU，将TPU 核（core）作为设备（device）。\n",
    "\n",
    "虽然一块TPU有多个核，本notebook只使用其中一个，后面我们会介绍如何使用多核。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc84bed",
   "metadata": {},
   "source": [
    "## 安装 PyTorch/XLA\n",
    "\n",
    "如果你使用的是Cloud TPU VM，不需要我们手动安装，在创建TPU VM时，可以直接选择预装的PyTorch环境，比如最新版的是\"tpu-vm-pt-1.12\"，已经安装了 PyTorch 1.12.0 and Pytorch / XLA 1.12.0。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c34c1d",
   "metadata": {},
   "source": [
    "## 在TPU上创建Tensor\n",
    "\n",
    "有了PyTorch/XLA，你可以像对待CPU或GPU那样来管理Cloud TPU，我们将每个Cloud TPU 核（core）都看作一个独立的PyTorch device。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da89dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports pytorch\n",
    "import torch\n",
    "\n",
    "# imports the torch_xla package\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed860673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu102 1.12\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__, torch_xla.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce91fe3",
   "metadata": {},
   "source": [
    "PyTorch/XLA (torch_xla)可以让PyTorch管理TPU device，函数 `xla_device()` 返回TPU的默认核作为device，下面就在TPU上创建一个tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f467efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], device='xla:1')\n"
     ]
    }
   ],
   "source": [
    "# Creates a random tensor on xla:1 (a Cloud TPU core)\n",
    "dev = xm.xla_device()\n",
    "t1 = torch.ones(3, 3, device = dev)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb65dd",
   "metadata": {},
   "source": [
    "如果执行上面的代码报错，\"RuntimeError: tensorflow/compiler/xla/xla_client/computation_client.cc:280 : Missing XLA configuration\"\n",
    "\n",
    "说明还没有配置XLA，只需要在终端执行\n",
    "`export XRT_TPU_CONFIG=\"localservice;0;localhost:51011\"`\n",
    "\n",
    "我们可以把这条命令添加到/etc/profile\n",
    "\n",
    "* 参考 https://pytorch-lightning.readthedocs.io/en/latest/accelerators/tpu_faq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7da6e7",
   "metadata": {},
   "source": [
    "可以看 http://pytorch.org/xla/ 中的文档，了解 PyTorch/XLA 都含有哪些函数。\n",
    "\n",
    "刚才使用了第一个TPU 核 ('xla:1')，我们切换另一个核:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34a419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='xla:2')\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor on the second Cloud TPU core\n",
    "second_dev = xm.xla_device(n=2, devkind='TPU')\n",
    "t2 = torch.zeros(3, 3, device = second_dev)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd88be9",
   "metadata": {},
   "source": [
    "建议使用 `xm.xla_device()` 来指定设备。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71adcaf8",
   "metadata": {},
   "source": [
    "TPU上创建的Tensor和其他Tensor 的用户体验完全相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ff0eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1846, -0.7140],\n",
      "        [-0.3259, -0.5264]], device='xla:1')\n",
      "tensor([[-0.9715, -1.2307],\n",
      "        [-2.1193,  0.7613]], device='xla:1')\n",
      "tensor([[ 0.4448,  0.3940],\n",
      "        [ 0.6057, -0.7984]], device='xla:1')\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2, device = dev)\n",
    "b = torch.randn(2, 2, device = dev)\n",
    "print(a + b)\n",
    "print(b * 2)\n",
    "print(torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a56aa",
   "metadata": {},
   "source": [
    "使用torch中的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68fea3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.2614,  -7.4375,  -3.0452,  ...,   6.4813,   6.0025,  -3.8181],\n",
       "         [ -2.1178,  -1.2323,   6.3152,  ...,  -8.1402,   1.5390,  10.5330],\n",
       "         [  3.7358,  -6.1666,  -5.3654,  ...,   3.9503,   6.6946,  -1.0387],\n",
       "         ...,\n",
       "         [ -1.0524,  -7.5402,  -6.6635,  ...,  -5.7106,  -9.5255,   9.1400],\n",
       "         [-12.9870,   1.4063,  -6.9533,  ...,  10.5729,   1.3097,  -5.2656],\n",
       "         [  5.0329,   1.4415,   8.1006,  ...,  -3.4235,   3.5638,  -5.9472]],\n",
       "\n",
       "        [[ -3.0059,   3.8605,   3.6280,  ...,  -8.1614, -13.1281,   5.2417],\n",
       "         [ -3.7675,  -4.9035,  -1.3131,  ...,   4.4226, -11.7430,  11.3242],\n",
       "         [-13.1958,   5.3812,   3.2664,  ...,  -4.4664,   5.2152,   2.1421],\n",
       "         ...,\n",
       "         [  5.9822,  -2.8872,   8.6605,  ...,  -9.1931,  -6.1449,  -6.7736],\n",
       "         [  1.4102,   1.8250, -12.2252,  ...,   3.3475,  -7.8704,   1.3273],\n",
       "         [  5.8575,  -0.6981,   3.5026,  ...,  -3.9181,   2.3322,   0.6250]],\n",
       "\n",
       "        [[ -7.5734,  -1.9799, -13.0047,  ...,  11.6987,   4.1112,  12.5896],\n",
       "         [  2.4195,  -1.4988,  -3.8268,  ...,   4.0141,   0.6266,  -3.1278],\n",
       "         [  4.9280,  -3.9038,   7.6932,  ...,  -5.2431,   9.5838,  10.3606],\n",
       "         ...,\n",
       "         [  7.3691,   1.8971, -17.9161,  ...,   2.2679,  -0.6399,   2.1474],\n",
       "         [  2.8902,  -6.0792,   3.1169,  ...,  -8.3092, -10.2779,   5.3017],\n",
       "         [ -8.7710,   4.8793,  -0.3555,  ...,   6.5064,  -0.1386,  -1.7103]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  4.6122,  -6.0387,  -5.9003,  ...,  12.0204,  -6.2949,  -3.9913],\n",
       "         [  9.1092,  -5.7928,   1.8601,  ...,   9.5592,  15.2506,  -0.7472],\n",
       "         [-10.9616,  -9.0558,   9.7424,  ...,  -4.7456,  -4.5659,  -2.2218],\n",
       "         ...,\n",
       "         [ -9.3438,   0.6222,  -5.2942,  ...,  11.9244,  -3.8386,  -0.8011],\n",
       "         [  9.8385,  -3.8062,  -4.0237,  ...,  -3.5106,  -6.2878,  -1.0366],\n",
       "         [ -0.4991,   4.2117,   0.1556,  ...,   1.6497,   3.3839, -12.5382]],\n",
       "\n",
       "        [[  1.6762,   1.6112,  -3.4230,  ...,  -4.8934,  -3.0786,   6.8579],\n",
       "         [  7.0891,  -0.8795,   6.5126,  ...,   6.4064,  -6.5050,  -2.8378],\n",
       "         [ -5.9176,  -3.2519,   5.2330,  ..., -12.9444,  -8.2811,  -3.4744],\n",
       "         ...,\n",
       "         [  2.6491,  -1.3777,   4.2195,  ...,  -2.5012,  -0.1836,  -2.1456],\n",
       "         [ -3.5724,  -2.5675,   3.6281,  ..., -21.3213,  -4.3397, -13.4994],\n",
       "         [  3.8137,   4.6518,   6.3794,  ...,  -3.3563,  -0.0448,   9.0587]],\n",
       "\n",
       "        [[-13.6712, -18.3582,   0.6431,  ...,   1.6914,   6.7037,  -0.9237],\n",
       "         [ -0.8231,  -0.9270,  -1.8442,  ...,  -1.2451,  -2.2990,  -0.2389],\n",
       "         [  0.3158,  -3.2141,  -2.5532,  ...,  -8.0853,  -6.0145,  -3.4992],\n",
       "         ...,\n",
       "         [  2.0704,  -8.7125, -17.9205,  ...,   5.2729,   4.0490,  -0.7910],\n",
       "         [ -0.7132,  -3.6804,   8.0226,  ...,   0.7933,  -9.6733,  -4.2041],\n",
       "         [ -0.2413,   3.4116,   8.6259,  ..., -11.2853,   4.6839,  -2.1019]]],\n",
       "       device='xla:1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates random filters and inputs to a 1D convolution\n",
    "filters = torch.randn(33, 16, 3, device = dev)\n",
    "inputs = torch.randn(20, 16, 50, device = dev)\n",
    "torch.nn.functional.conv1d(inputs, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b150f",
   "metadata": {},
   "source": [
    "tensors也可以在CPU和TPU之间传输，但是注意PyTorch跨设备传输tensor都是传输的备份（copy）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfe84a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1306, -0.2727],\n",
      "        [-0.5207,  0.3036]])\n",
      "tensor([[-1.1306, -0.2727],\n",
      "        [-0.5207,  0.3036]], device='xla:1')\n",
      "tensor([[-1.1306, -0.2727],\n",
      "        [-0.5207,  0.3036]])\n"
     ]
    }
   ],
   "source": [
    "# 在CPU上创建一个tensor，\n",
    "t_cpu = torch.randn(2, 2, device='cpu')\n",
    "print(t_cpu)\n",
    "\n",
    "# 将t_cpu传输到TPU，注意：实际上传输的是备份\n",
    "t_tpu = t_cpu.to(dev)\n",
    "print(t_tpu)\n",
    "\n",
    "# 将t_tpu传输到CPU，注意：实际上传输的是备份\n",
    "t_cpu_again = t_tpu.to('cpu')\n",
    "print(t_cpu_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ac2f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140206247038624, 140208887314928)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(t_cpu), id(t_cpu_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f9492",
   "metadata": {},
   "source": [
    "可以看到，这是两个tensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca16c8d",
   "metadata": {},
   "source": [
    "## Running PyTorch modules and autograd on TPUs\n",
    "\n",
    "Modules 和 autograd 是PyTorch的基础，可以无缝操纵TPU tensor。\n",
    "\n",
    "PyTorch中每个有状态的函数都对应一个同功能的Module，Module是一个类，封装了数据和方法。比如线性层是一个module，由于Module是有状态的，所以可以放在device中:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86398df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9235, -1.2827],\n",
      "        [-0.6867,  0.3928],\n",
      "        [-1.4930,  0.7258]], device='xla:1', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个线性module\n",
    "fc = torch.nn.Linear(5, 2, bias=True)\n",
    "\n",
    "# 将这个module拷贝到TPU核中\n",
    "fc = fc.to(dev)\n",
    "\n",
    "# Creates a random feature tensor\n",
    "features = torch.randn(3, 5, device=dev, requires_grad=True)\n",
    "\n",
    "# Runs and prints the module\n",
    "output = fc(features)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7786136",
   "metadata": {},
   "source": [
    "Autograd 是PyTorch中的自动微分系统，如果一个Module在TPU核中，那么Module中参数的梯度也在同一个TPU核上:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14061573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6606,  1.4688,  1.9482,  0.6864, -1.0761],\n",
      "        [ 1.6606,  1.4688,  1.9482,  0.6864, -1.0761]], device='xla:1')\n"
     ]
    }
   ],
   "source": [
    "output.backward(torch.ones_like(output))\n",
    "print(fc.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f986521",
   "metadata": {},
   "source": [
    "## 在TPU上运行神经网络\n",
    "\n",
    "既然Module可以放在TPU上，NN当然也可以，毕竟也是modules："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1393784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0107, -0.0980,  0.1534,  0.0691,  0.0965,  0.0634, -0.0321,  0.0635,\n",
      "          0.0590, -0.0895]], device='xla:1', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simple example network from \n",
    "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "# Places network on the default TPU core\n",
    "net = Net().to(dev)\n",
    "\n",
    "# Creates random input on the default TPU core\n",
    "input = torch.randn(1, 1, 32, 32, device=dev)\n",
    "\n",
    "# Runs network\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd05cd",
   "metadata": {},
   "source": [
    "就是如此简单，只需要把device指定为TPU核即可。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
